var documenterSearchIndex = {"docs":
[{"location":"nonparametric/#Non-parametric-tests","page":"Non-Parametric Tests","title":"Non-parametric tests","text":"","category":"section"},{"location":"nonparametric/","page":"Non-Parametric Tests","title":"Non-Parametric Tests","text":"Multiple non-parametric tests. Here, Pingouin is mostly a wrapper around HypothesisTests.jl.","category":"page"},{"location":"nonparametric/","page":"Non-Parametric Tests","title":"Non-Parametric Tests","text":"Modules = [Pingouin]\nPages   = [\"nonparametric.jl\"]","category":"page"},{"location":"nonparametric/#Pingouin.cochran-Tuple{DataFrames.DataFrame}","page":"Non-Parametric Tests","title":"Pingouin.cochran","text":"cochran(data[, dv, within, subject])\n\nCochran Q test. A special case of the Friedman test when the dependent variable is binary.\n\nArguments\n\ndata::DataFrame\ndv::Union{Nothing,String,Symbol}: Name of column containing the binary dependent variable.\nwithin::Union{Nothing,String,Symbol}: Name of column containing the within-subject factor.\nsubject::Union{Nothing,String,Symbol}: Name of column containing the subject identifier.\n\nReturns\n\nstats::DataFrame\n'Q': The Cochran Q statistic,\n'p-unc': Uncorrected p-value,\n'ddof': degrees of freedom.\n\nNotes\n\nThe Cochran Q test [1] is a non-parametric test for ANOVA with repeated measures where the dependent variable is binary.\n\nData are expected to be in long-format. NaN are automatically removed from the data.\n\nThe Q statistics is defined as:\n\nQ = frac(r-1)(rsum_j^rx_j^2-N^2)rN-sum_i^nx_i^2\n\nwhere N is the total sum of all observations, j=1r where r is the number of repeated measures, i=1n where n is the number of observations per condition.\n\nThe p-value is then approximated using a chi-square distribution with r-1 degrees of freedom:\n\nQ sim chi^2(r-1)\n\nReferences\n\n[1] Cochran, W.G., 1950. The comparison of percentages in matched samples. Biometrika 37, 256–266. https://doi.org/10.1093/biomet/37.3-4.256\n\nExamples\n\nCompute the Cochran Q test for repeated measurements.\n\njulia> data = Pingouin.read_dataset(\"cochran\");\njulia> cochran(data, dv=\"Energetic\", within=\"Time\", subject=\"Subject\")\n1×4 DataFrame\n│ Row │ Source │ ddof  │ Q       │ p_unc     │\n│     │ String │ Int64 │ Float64 │ Float64   │\n├─────┼────────┼───────┼─────────┼───────────┤\n│ 1   │ Time   │ 2     │ 6.70588 │ 0.0349813 │\n\n\n\n\n\n","category":"method"},{"location":"nonparametric/#Pingouin.friedman-Tuple{DataFrames.DataFrame}","page":"Non-Parametric Tests","title":"Pingouin.friedman","text":"friedman(data, dv, within, subject, method)\n\nFriedman test for repeated measurements.\n\nArguments\n\ndata::DataFrame,\ndv::Union{String,Symbol}: Name of column containing the dependent variable,\nwithin::Union{String,Symbol}: Name of column containing the within-subject factor,\nsubject::Union{String,Symbol}: Name of column containing the subject identifier,\nmethod::String: Statistical test to perform. Must be \"chisq\" (chi-square test) or \"f\" (F test).\n\nSee notes below for explanation.\n\nReturns\n\n\"W\": Kendall's coefficient of concordance, corrected for ties,\nstats::DataFrame, if method=\"chisq\"\n\"Q\": The Friedman Q statistic, corrected for ties,\n\"p-unc\": Uncorrected p-value,\n\"ddof\": degrees of freedom.\nstats::DataFrame, if method=\"f\":\n\"F\": The Friedman F statistic, corrected for ties,\n\"p-unc\": Uncorrected p-value,\n\"ddof1\": degrees of freedom of the numerator,\n\"ddof2\": degrees of freedom of the denominator.\n\nNotes\n\nThe Friedman test is used for one-way repeated measures ANOVA by ranks.\n\nData are expected to be in long-format.\n\nNote that if the dataset contains one or more other within subject factors, an automatic collapsing to the mean is applied on the dependent variable (same behavior as the ezANOVA R package). As such, results can differ from those of JASP. If you can, always double-check the results.\n\nDue to the assumption that the test statistic has a chi squared distribution, the p-value is only reliable for n > 10 and more than 6 repeated measurements.\n\nNaN values are automatically removed.\n\nThe Friedman test is equivalent to the test of significance of Kendalls's coefficient of concordance (Kendall's W). Most commonly a Q statistic, which has asymptotical chi-squared distribution, is computed and used for testing. However, in [1] they showed the chi-squared test to be overly conservative for small numbers of samples and repeated measures. Instead they recommend the F test, which has the correct size and behaves like a permutation test, but is computationaly much easier.\n\nReferences\n\n[1] Marozzi, M. (2014). Testing for concordance between several     criteria. Journal of Statistical Computation and Simulation,     84(9), 1843–1850. https://doi.org/10.1080/00949655.2013.766189\n\nExamples\n\nCompute the Friedman test for repeated measurements.\n\njulia> data = Pingouin.read_dataset(\"rm_anova\")\njulia> Pingouin.friedman(data,\n                         dv=\"DesireToKill\",\n                         within=\"Disgustingness\",\n                         subject=\"Subject\")\n1×5 DataFrame\n Row │ Source          W          ddof   Q        p_unc      \n     │ String          Float64    Int64  Float64  Float64    \n─────┼───────────────────────────────────────────────────────\n   1 │ Disgustingness  0.0992242      1  9.22785  0.00238362\n\nThis time we will use the F test method.\n\njulia> data = Pingouin.read_dataset(\"rm_anova\")\njulia> Pingouin.friedman(data,\n                         dv=\"DesireToKill\",\n                         within=\"Disgustingness\",\n                         subject=\"Subject\",\n                         method=\"f\")\n1×6 DataFrame\n Row │ Source          W          ddof1     ddof2    F        p_unc      \n     │ String          Float64    Float64   Float64  Float64  Float64    \n─────┼───────────────────────────────────────────────────────────────────\n   1 │ Disgustingness  0.0992242  0.978495  90.0215  10.1342  0.00213772\n\n\n\n\n\n","category":"method"},{"location":"nonparametric/#Pingouin.harrelldavis","page":"Non-Parametric Tests","title":"Pingouin.harrelldavis","text":"harrelldavis(x[, q, dim])\n\nEXPERIMENTAL Harrell-Davis robust estimate of the q^th quantile(s) of the data. TESTS NEEDED\n\nArguments\n\nx::Array{<:Number}: Data, must be a one or two-dimensional vector.\nq::Union{Float64,Array{Float64}}: Quantile or sequence of quantiles to compute, must be between 0 and 1. Default is 05.\ndim::Int64: Axis along which the MAD is computed. Default is the first axis. Can be either 1 or 2.\n\nReturns\n\ny::Union{Float64,Array{Float64}}: The estimated quantile(s). If quantile is a single quantile, will return a float, otherwise will compute each quantile separately and returns an array of floats.\n\nNotes\n\nThe Harrell-Davis method [1] estimates the q^th quantile by a linear combination of  the  order statistics. Results have been tested against a Matlab implementation [2]. Note that this method is also used to measure the confidence intervals of the difference between quantiles of two groups, as implemented in the shift function [3].\n\nSee Also\n\nplot_shift\n\nReferences\n\n[1] Frank E. Harrell, C. E. Davis, A new distribution-free quantile estimator, Biometrika, Volume 69, Issue 3, December 1982, Pages 635–640, https://doi.org/10.1093/biomet/69.3.635\n\n[2] https://github.com/GRousselet/matlab_stats/blob/master/hd.m\n\n[3] Rousselet, G. A., Pernet, C. R. and Wilcox, R. R. (2017). Beyond differences in means: robust graphical methods to compare two groups in neuroscience. Eur J Neurosci, 46: 1738-1748. https://doi.org/doi:10.1111/ejn.13610\n\nExamples\n\nEstimate the 0.5 quantile (i.e median) of 100 observation picked from a normal distribution with zero mean and unit variance.\n\njulia> using Distributions, Random\njulia> d = Normal(0, 1)\njulia> x = rand(d, 100);\n>>> Pingouin.harrelldavis(x, 0.5)\n-0.3197175569523778\n\nSeveral quantiles at once\n\njulia> Pingouin.harrelldavis(x, [0.25, 0.5, 0.75])\n3-element Array{Float64,1}:\n -0.8584761447019648\n -0.3197175569523778\n  0.30049291160713604\n\nOn the last axis of a 2D vector (default)\n\njulia> using Distributions, Random\njulia> d = Normal(0, 1)\njulia> x = rand(d, (100, 100));\njulia> Pingouin.harrelldavis(x, 0.5)\n100×1 Array{Float64,2}:\n  0.08776830864191214\n  0.03470963005927001\n -0.0805646920967012\n  0.3314919956251108\n  0.3111971350475172\n  ⋮\n  0.10769293112437549\n -0.10622118136247076\n -0.13230506142402296\n -0.09693123033727057\n -0.2135938540892071\n\nOn the first axis\n\njulia> Pingouin.harrelldavis(x, 0.5, 1)\n1×100 Array{Float64,2}:\n 0.0112259  -0.0409635  -0.0918462 ...\n\nOn the first axis with multiple quantiles\n\njulia> Pingouin.harrelldavis(x, [0.5, 0.75], 1)\n1×100 Array{Float64,2}:\n 0.0112259  -0.0409635  -0.0918462 ...\n\n\n\n\n\n","category":"function"},{"location":"nonparametric/#Pingouin.kruskal-Tuple{Any}","page":"Non-Parametric Tests","title":"Pingouin.kruskal","text":"kruskal(data[, dv, between, detailed])\n\nKruskal-Wallis H-test for independent samples.\n\nArguments\n\ndata::DataFrame: DataFrame,\ndv::String: Name of column containing the dependent variable,\nbetween::String: Name of column containing the between factor.\n\nReturns\n\nstats::DataFrame\n'H': The Kruskal-Wallis H statistic, corrected for ties,\n'p-unc': Uncorrected p-value,\n'dof': degrees of freedom.\n\nNotes\n\nThe Kruskal-Wallis H-test tests the null hypothesis that the population median of all of the groups are equal. It is a non-parametric version of ANOVA. The test works on 2 or more independent samples, which may have different sizes.\n\nDue to the assumption that H has a chi square distribution, the number of samples in each group must not be too small. A typical rule is that each sample must have at least 5 measurements.\n\nNaN values are automatically removed.\n\nExamples\n\nCompute the Kruskal-Wallis H-test for independent samples.\n\njulia> data = Pingouin.read_dataset(\"anova\")\njulia> Pingouin.kruskal(data, dv=\"Pain threshold\", between=\"Hair color\")\n1×4 DataFrame\n│ Row │ Source     │ ddof  │ H       │ p_unc     │\n│     │ String     │ Int64 │ Float64 │ Float64   │\n├─────┼────────────┼───────┼─────────┼───────────┤\n│ 1   │ Hair color │ 3     │ 10.5886 │ 0.0141716 │\n\n\n\n\n\n","category":"method"},{"location":"nonparametric/#Pingouin.madmedianrule-Tuple{Array{var\"#s176\",N} where N where var\"#s176\"<:Number}","page":"Non-Parametric Tests","title":"Pingouin.madmedianrule","text":"madmedianrule(a)\n\nRobust outlier detection based on the MAD-median rule.\n\nArguments\n\na::Array{<:Number}: Input array. Must be one-dimensional.\n\nReturns\n\noutliers::Array{Bool}: Boolean array indicating whether each sample is an outlier (true) or not (false).\n\nSee also\n\nStatistics.mad\n\nNotes\n\nThe MAD-median-rule ([1], [2]) will refer to declaring X_i an outlier if\n\nfracleft  X_i - M right textMAD_textnorm  K\n\n,\n\nwhere M is the median of X, textMAD_textnorm the normalized median absolute deviation of X, and K is the square root of the .975 quantile of a X^2 distribution with one degree of freedom, which is roughly equal to 2.24.\n\nReferences\n\n[1] Hall, P., Welsh, A.H., 1985. Limit theorems for the median deviation. Ann. Inst. Stat. Math. 37, 27–36. https://doi.org/10.1007/BF02481078\n\n[2] Wilcox, R. R. Introduction to Robust Estimation and Hypothesis Testing. (Academic Press, 2011).\n\nExamples\n\njulia> a = [-1.09, 1., 0.28, -1.51, -0.58, 6.61, -2.43, -0.43]\njulia> Pingouin.madmedianrule(a)\n8-element Array{Bool,1}:\n 0\n 0\n 0\n 0\n 0\n 1\n 0\n 0\n\n\n\n\n\n","category":"method"},{"location":"nonparametric/#Pingouin.mwu-Tuple{Array{var\"#s175\",N} where N where var\"#s175\"<:Number,Array{var\"#s174\",N} where N where var\"#s174\"<:Number}","page":"Non-Parametric Tests","title":"Pingouin.mwu","text":"mwu(x, y)\n\nMann-Whitney U Test (= Wilcoxon rank-sum test). It is the non-parametric version of the independent T-test.\n\nArguments\n\nx, y::Array{<:Number}: First and second set of observations. x and y must be independent.\n\nReturns\n\nstats::DataFrame\n'U-val': U-value\n'p-val': p-value\n'RBC': rank-biserial correlation\n'CLES': common language effect size\n\nSee also\n\nHypothesisTests.MannWhitneyUTest,\nwilcoxon,\nttest.\n\nNotes\n\nThe Mann–Whitney U test [1], (also called Wilcoxon rank-sum test) is a non-parametric test of the null hypothesis that it is equally likely that a randomly selected value from one sample will be less than or greater than a randomly selected value from a second sample. The test assumes that the two samples are independent. This test corrects for ties and by default uses a continuity correction (see HypothesisTests.MannWhitneyUTest for details).\n\nThe rank biserial correlation [2] is the difference between the proportion of favorable evidence minus the proportion of unfavorable evidence.\n\nThe common language effect size is the proportion of pairs where x is higher than y. It was first introduced by McGraw and Wong (1992) [3]. Pingouin uses a brute-force version of the formula given by Vargha and Delaney 2000 [4]:\n\ntextCL = P(X  Y) + 5 times P(X = Y)\n\nThe advantage is of this method are twofold. First, the brute-force approach pairs each observation of x to its y counterpart, and therefore does not require normally distributed data. Second, the formula takes ties into account and therefore works with ordinal data.\n\nReferences\n\n[1] Mann, H. B., & Whitney, D. R. (1947). On a test of whether one of two random variables is stochastically larger than the other. The annals of mathematical statistics, 50-60.\n\n[2] Kerby, D. S. (2014). The simple difference formula: An approach to teaching nonparametric correlation. Comprehensive Psychology, 3, 11-IT.\n\n[3] McGraw, K. O., & Wong, S. P. (1992). A common language effect size statistic. Psychological bulletin, 111(2), 361.\n\n[4] Vargha, A., & Delaney, H. D. (2000). A Critique and Improvement of the “CL” Common Language Effect Size Statistics of McGraw and Wong. Journal of Educational and Behavioral Statistics: A Quarterly Publication Sponsored by the American Educational Research Association and the American Statistical Association, 25(2), 101–132. https://doi.org/10.2307/1165329\n\nExamples\n\njulia> x = [1,4,2,5,3,6,9,8,7]\njulia> y = [2,4,1,5,10,1,4,9,8,5]\njulia> Pingouin.mwu(x, y)\n1×4 DataFrame\n│ Row │ U_val   │ p_val    │ RBC        │ CLES     │\n│     │ Float64 │ Float64  │ Float64    │ Float64  │\n├─────┼─────────┼──────────┼────────────┼──────────┤\n│ 1   │ 46.5    │ 0.934494 │ -0.0333333 │ 0.516667 │\n\nCompare with HypothesisTests\n\njulia> using HypothesisTests\njulia> MannWhitneyUTest(x, y)\nApproximate Mann-Whitney U test\n-------------------------------\nPopulation details:\n    parameter of interest:   Location parameter (pseudomedian)\n    value under h_0:         0\n    point estimate:          0.5\n\nTest summary:\n    outcome with 95% confidence: fail to reject h_0\n    two-sided p-value:           0.9345\n\nDetails:\n    number of observations in each group: [9, 10]\n    Mann-Whitney-U statistic:             46.5\n    rank sums:                            [91.5, 98.5]\n    adjustment for ties:                  90.0\n    normal approximation (μ, σ):          (1.5, 12.1666)\n\n\n\n\n\n","category":"method"},{"location":"nonparametric/#Pingouin.wilcoxon-Tuple{Array{var\"#s175\",N} where N where var\"#s175\"<:Number,Array{var\"#s174\",N} where N where var\"#s174\"<:Number}","page":"Non-Parametric Tests","title":"Pingouin.wilcoxon","text":"wilcoxon(x, y)\n\nWilcoxon signed-rank test. It is the non-parametric version of the paired T-test.\n\nArguments\n\nx, y::Array{<:Number}: First and second set of observations. x and y must be related (e.g repeated measures) and, therefore, have the same number of samples. Note that a listwise deletion of missing values is automatically applied.\n\nReturns\n\nstats::DataFrame\n'W-val': W-value\n'p-val': p-value\n'RBC': matched pairs rank-biserial correlation (effect size)\n'CLES': common language effect size\n\nSee also\n\nHypothesisTests.SignedRankTest,\nmwu.\n\nNotes\n\nThe Wilcoxon signed-rank test [1] tests the null hypothesis that two related paired samples come from the same distribution. In particular, it tests whether the distribution of the differences x - y is symmetric about zero. A continuity correction is applied by default (see HypothesisTests.SignedRankTest for details).\n\nThe matched pairs rank biserial correlation [2] is the simple difference between the proportion of favorable and unfavorable evidence; in the case of the Wilcoxon signed-rank test, the evidence consists of rank sums (Kerby 2014):\n\nr = f - u\n\nThe common language effect size is the proportion of pairs where x is higher than y. It was first introduced by McGraw and Wong (1992) [3]. Pingouin uses a brute-force version of the formula given by Vargha and Delaney 2000 [4]:\n\ntextCL = P(X  Y) + 5 times P(X = Y)\n\nThe advantage is of this method are twofold. First, the brute-force approach pairs each observation of x to its y counterpart, and therefore does not require normally distributed data. Second, the formula takes ties into account and therefore works with ordinal data.\n\nReferences\n\n[1] Wilcoxon, F. (1945). Individual comparisons by ranking methods. Biometrics bulletin, 1(6), 80-83.\n\n[2] Kerby, D. S. (2014). The simple difference formula: An approach to teaching nonparametric correlation. Comprehensive Psychology, 3, 11-IT.\n\n[3] McGraw, K. O., & Wong, S. P. (1992). A common language effect size statistic. Psychological bulletin, 111(2), 361.\n\n[4] Vargha, A., & Delaney, H. D. (2000). A Critique and Improvement of the “CL” Common Language Effect Size Statistics of McGraw and Wong. Journal of Educational and Behavioral Statistics: A Quarterly Publication Sponsored by the American Educational Research Association and the American Statistical Association, 25(2), 101–132. https://doi.org/10.2307/1165329\n\nExamples\n\nWilcoxon test on two related samples.\n\njulia> x = [20, 22, 19, 20, 22, 18, 24, 20, 19, 24, 26, 13]\njulia> y = [38, 37, 33, 29, 14, 12, 20, 22, 17, 25, 26, 16]\njulia> Pingouin.wilcoxon(x, y)\n1×4 DataFrame\n│ Row │ W_val   │ p_val    │ RBC       │ CLES     │\n│     │ Float64 │ Float64  │ Float64   │ Float64  │\n├─────┼─────────┼──────────┼───────────┼──────────┤\n│ 1   │ 20.5    │ 0.288086 │ -0.378788 │ 0.395833 │\n\nCompare with HypothesisTests\n\njulia> using HypothesisTests\njulia> SignedRankTest(x, y)\nExact Wilcoxon signed rank test\n-------------------------------\nPopulation details:\n    parameter of interest:   Location parameter (pseudomedian)\n    value under h_0:         0\n    point estimate:          -1.5\n    95% confidence interval: (-9.0, 2.5)\n\nTest summary:\n    outcome with 95% confidence: fail to reject h_0\n    two-sided p-value:           0.2881\n\nDetails:\n    number of observations:      12\n    Wilcoxon rank-sum statistic: 20.5\n    rank sums:                   [20.5, 45.5]\n    adjustment for ties:         6.0\n\n\n\n\n\n","category":"method"},{"location":"bayesian/#Bayesian","page":"Bayesian","title":"Bayesian","text":"","category":"section"},{"location":"bayesian/","page":"Bayesian","title":"Bayesian","text":"Bayesian tests, mostly focused around BF10 and BF01.","category":"page"},{"location":"bayesian/","page":"Bayesian","title":"Bayesian","text":"Modules = [Pingouin]\nPages   = [\"bayesian.jl\"]","category":"page"},{"location":"bayesian/#Pingouin.bayesfactor_binom","page":"Bayesian","title":"Pingouin.bayesfactor_binom","text":"bayesfactor_binom(k, n, p)\n\nBayes factor of a binomial test with k successes, n trials and base probability p.\n\nArguments\n\nk::Int64: Number of successes.\nn::Int64: Number of trials.\np::Float64: Base probability of success (range from 0 to 1).\n\nReturns\n\nbinom_bf::Float64: The Bayes Factor quantifies the evidence in favour of the alternative hypothesis, where the null hypothesis is that the random variable is binomially distributed with base probability p.\n\nSee also\n\nbayesfactor_pearson: Bayes Factor of a correlation\nbayesfactor_ttest: Bayes Factor of a T-test\n\nNotes\n\nAdapted from a Matlab code found at https://github.com/anne-urai/Tools/blob/master/stats/BayesFactors/binombf.m The Bayes Factor is given by the formula below:\n\nBF_10 = fracint_0^1 binomnkg^k(1-g)^n-k binomnk p^k (1-p)^n-k\n\nReferences\n\nhttp://pcl.missouri.edu/bf-binomial\nhttps://en.wikipedia.org/wiki/Bayes_factor\n\nExamples\n\nWe want to determine if a coin is fair. After tossing the coin 200 times in a row, we report 115 heads (hereafter referred to as \"successes\") and 85 tails (\"failures\"). The Bayes Factor can be easily computed using Pingouin:\n\njulia> using Pingouin\njulia> bf = Pingouin.bayesfactor_binom(115, 200, 0.5)\njulia> # Note that Pingouin returns the BF-alt by default.\njulia> # BF-null is simply 1 / BF-alt\njulia> print(\"BF-null: \", 1 / bf, \"; BF-alt: \", bf)\nBF-null: 1.197134330237552; BF-alt: 0.8353281455069175\n\nSince the Bayes Factor of the null hypothesis (\"the coin is fair\") is higher than the Bayes Factor of the alternative hypothesis (\"the coin is not fair\"), we can conclude that there is more evidence to support the fact that the coin is indeed fair. However, the strength of the evidence in favor of the null hypothesis (1.197) is \"barely worth mentionning\" according to Jeffreys's rule of thumb.\n\nInterestingly, a frequentist alternative to this test would give very different results. It can be performed using the SciPy.stats.binom_test function:\n\njulia> using SciPy\njulia> pval = SciPy.stats.binom_test(115, 200, p=0.5)\njulia> round.(pval, digits=5)\n0.04004\n\nThe binomial test rejects the null hypothesis that the coin is fair at the 5% significance level (p=0.04). Thus, whereas a frequentist hypothesis test would yield significant results at the 5% significance level, the Bayes factor does not find any evidence that the coin is unfair. Last example using a different base probability of successes\n\njulia> bf = Pingouin.bayesfactor_binom(100, 1000, 0.1)\njulia> print(\"Bayes Factor: \", round.(bf, digits=3))\nBayes Factor: 0.024\n\n\n\n\n\n","category":"function"},{"location":"bayesian/#Pingouin.bayesfactor_pearson-Tuple{Float64,Int64}","page":"Bayesian","title":"Pingouin.bayesfactor_pearson","text":"bayesfactor_pearson(r, n[, tail, method, kappa])\n\nBayes Factor of a Pearson correlation.\n\nArguments\n\nr::Float64: Pearson correlation coefficient.\nn::Int64: Sample size.\ntail::String: Tail of the alternative hypothesis. Can be 'two-sided', 'one-sided', 'greater' or 'less'. 'greater' corresponds to a positive correlation, 'less' to a negative correlation. If 'one-sided', the directionality is inferred based on the r value (= 'greater' if r > 0, 'less' if r < 0).\nmethod::String: Method to compute the Bayes Factor. Can be 'ly' (default) or 'wetzels'. The former has an exact analytical solution, while the latter requires integral solving (and is therefore slower). 'wetzels' was the default in Pingouin <= 0.2.5. See Notes for details.\nkappa::Float64: Kappa factor. This is sometimes called the rscale parameter, and\n\nis only used when method is 'ly'.\n\nReturns\n\nbf::Float64: Bayes Factor (BF10).\n\nThe Bayes Factor quantifies the evidence in favour of the alternative hypothesis.\n\nSee also\n\ncorr: (Robust) correlation between two variables,\npairwise_corr: Pairwise correlation between columns of a pandas DataFrame,\nbayesfactor_ttest: Bayes Factor of a T-test,\nbayesfactor_binom: Bayes Factor of a binomial test.\n\nNotes\n\nTo compute the Bayes Factor directly from the raw data, use the pingouin.corr function. The two-sided Wetzels Bayes Factor (also called JZS Bayes Factor) is calculated using the equation 13 and associated R code of [1]:\n\ntextBF_10(n r) = fracsqrtn2gamma(12)* int_0^inftye((n-2)2)* log(1+g)+(-(n-1)2)log(1+(1-r^2)*g)+(-32)log(g)-n2g\n\nwhere n is the sample size, r is the Pearson correlation coefficient and g is is an auxiliary variable that is integrated out numerically. Since the Wetzels Bayes Factor requires solving an integral, it is slower than the analytical solution described below. The two-sided Ly Bayes Factor (also called Jeffreys exact Bayes Factor) is calculated using equation 25 of [2]:\n\ntextBF_10k(n r) = frac2^frack-2ksqrtpi beta(frac1k frac1k) cdot fracGamma(frac2+k(n-1)2k)Gamma(frac2+nk2k) cdot 2F_1(fracn-12 fracn-12 frac2+nk2k r^2)\n\nThe one-sided version is described in eq. 27 and 28 of Ly et al, 2016. Please take note that the one-sided test requires the mpmath package. Results have been validated against JASP and the BayesFactor R package.\n\nReferences\n\n[1] Ly, A., Verhagen, J. & Wagenmakers, E.-J. Harold Jeffreys’s default Bayes factor hypothesis tests: Explanation, extension, and application in psychology. J. Math. Psychol. 72, 19–32 (2016).\n\n[2] Wetzels, R. & Wagenmakers, E.-J. A default Bayesian hypothesis test for correlations and partial correlations. Psychon. Bull. Rev. 19, 1057–1064 (2012).\n\nExamples\n\nBayes Factor of a Pearson correlation\n\njulia> using Pingouin\njulia> r, n = 0.6, 20\njulia> bf = Pingouin.bayesfactor_pearson(r, n)\njulia> print(\"Bayes Factor: \", round.(bf, digits=3))\nBayes Factor: 10.634\n\nCompare to Wetzels method:\n\njulia> bf = Pingouin.bayesfactor_pearson(r, n,\n                                    tail=\"two-sided\",\n                                    method=\"wetzels\",\n                                    kappa=1.)\njulia> print(\"Bayes Factor: \", round.(bf, digits=3))\nBayes Factor: 8.221\n\nOne-sided test\n\njulia> bf10pos = Pingouin.bayesfactor_pearson(r, n, \n                                         tail=\"one-sided\",\n                                         method=\"ly\",\n                                         kappa=1.0)\njulia> bf10neg = Pingouin.bayesfactor_pearson(r, n,\n                                         tail=\"less\",\n                                         method=\"ly\",\n                                         kappa=1.0)\njulia> print(\"BF-pos: \", round.(bf10pos, digits=3),\" BF-neg: \", round.(bf10neg, digits=3))\nBF-pos: 21.185, BF-neg: 0.082\n\nWe can also only pass tail='one-sided' and Pingouin will automatically infer the directionality of the test based on the r value.\n\njulia> print(\"BF: \", round.(Pingouin.bayesfactor_pearson(r, n, tail=\"one-sided\"), digits=3))\nBF: 21.185\n\n\n\n\n\n","category":"method"},{"location":"bayesian/#Pingouin.bayesfactor_ttest","page":"Bayesian","title":"Pingouin.bayesfactor_ttest","text":"bayesfactor_ttest(t, nx[, ny, paired, tail, r])\n\nBayes Factor of a T-test.\n\nArguments\n\nt::Float64: T-value of the T-test\nnx::Int64: Sample size of first group\nny::Int64: Sample size of second group (only needed in case of an independent two-sample T-test)\npaired::Bool: boolean Specify whether the two observations are related (i.e. repeated measures) or independent.\ntail::String: string Specify whether the test is 'one-sided' or 'two-sided'. Can also be 'greater' or 'less' to specify the direction of the test.\n\nWARNING: One-sided Bayes Factor (BF) are simply obtained by doubling the two-sided BF, which is not exactly the same behavior as R or JASP. Be extra careful when interpretating one-sided BF, and if you can, always double-check your results.\n\nr::Float64: Cauchy scale factor. Smaller values of r (e.g. 0.5), may be appropriate when small effect sizes are expected a priori; larger values of r are appropriate when large effect sizes are expected (Rouder et al 2009). The default is sqrt2  2 approx 0707.\n\nReturns\n\nbf::Float64: Scaled Jeffrey-Zellner-Siow (JZS) Bayes Factor (BF10). The Bayes Factor quantifies the evidence in favour of the alternative hypothesis.\n\nSee also\n\nttest: T-test\npairwise_ttest: Pairwise T-tests\nbayesfactor_pearson: Bayes Factor of a correlation\nbayesfactor_binom: Bayes Factor of a binomial test\n\nNotes\n\nAdapted from a Matlab code found at https://github.com/anne-urai/Tools/tree/master/stats/BayesFactors If you would like to compute the Bayes Factor directly from the raw data instead of from the T-value, use the pingouin.ttest function. The JZS Bayes Factor is approximated using the formula described in ref [1]:\n\ntextBF_10 = fracint_0^infty(1 + Ngr^2)^-12 (1 + fract^2v(1 + Ngr^2))^-(v+1)  2(2pi)^-12g^ -32e^-12g(1 + fract^2v)^-(v+1)  2\n\nwhere t is the T-value, v the degrees of freedom, N the sample size, r the Cauchy scale factor (= prior on effect size) and g is is an auxiliary variable that is integrated out numerically. Results have been validated against JASP and the BayesFactor R package.\n\nReferences\n\n[1] Rouder, J.N., Speckman, P.L., Sun, D., Morey, R.D., Iverson, G.,\n\nBayesian t tests for accepting and rejecting the null hypothesis.\n\nPsychon. Bull. Rev. 16, 225–237. https://doi.org/10.3758/PBR.16.2.225\n\nExamples\n\nBayes Factor of an independent two-sample T-test\n\njulia> bf = Pingouin.bayesfactor_ttest(3.5, 20, 20)\njulia> print(\"Bayes Factor: \", round.(bf, digits=3), \" (two-sample independent)\")\nBayes Factor: 26.743 (two-sample independent)\n\nBayes Factor of a paired two-sample T-test\n\njulia> bf = Pingouin.bayesfactor_ttest(3.5, 20, 20, paired=true)\njulia> print(\"Bayes Factor: \", round.(bf, digits=3), \" (two-sample paired)\")\nBayes Factor: 17.185 (two-sample paired)\n\nBayes Factor of an one-sided one-sample T-test\n\njulia> bf = Pingouin.bayesfactor_ttest(3.5, 20, tail=\"one-sided\")\njulia> print(\"Bayes Factor: \", round.(bf, digits=3), \" (one-sample)\")\nBayes Factor: 34.369 (one-sample)\n\nNow specifying the direction of the test\n\njulia> tval = -3.5\njulia> bf_greater = Pingouin.bayesfactor_ttest(tval, 20, tail=\"greater\")\njulia> bf_less = Pingouin.bayesfactor_ttest(tval, 20, tail=\"less\")\njulia> print(\"BF10-greater: \", round.(bf_greater, digits=3), \" | BF10-less: \", round.(bf_less, digits=3))\nBF10-greater: 0.029 | BF10-less: 34.369\n\n\n\n\n\n","category":"function"},{"location":"distributions/#Distributions","page":"Distributions","title":"Distributions","text":"","category":"section"},{"location":"distributions/","page":"Distributions","title":"Distributions","text":"Statistical tests for multiple distributions, and tests for homoscedasticity.","category":"page"},{"location":"distributions/","page":"Distributions","title":"Distributions","text":"Modules = [Pingouin]\nPages   = [\"distributions.jl\"]","category":"page"},{"location":"distributions/#Pingouin.anderson","page":"Distributions","title":"Pingouin.anderson","text":"anderson(x[, dist])\n\nAnderson-Darling test of distribution.\n\nArguments\n\nx::Array{<:Number}: Array of sample data. May be different lengths. Also support multiple arrays (see examples),\ndist::Union{String, Distribution}: Distribution (\"norm\", \"expon\", \"logistic\", \"gumbel\").\n\nReturns\n\nH::Bool: True if data comes from this distribution,\nP::Float64: The significance levels for the corresponding critical values in %.\n\n(See :HypothesisTests.OneSampleADTest for more details).\n\nExamples\n\nTest that an array comes from a normal distribution\n\njulia> x = [2.3, 5.1, 4.3, 2.6, 7.8, 9.2, 1.4]\njulia> Pingouin.anderson(x, \"norm\")\n(false, 8.571428568870942e-5)\n\nTest that an array comes from a custom distribution\n\njulia> using Distributions\njulia> x = [2.3, 5.1, 4.3, 2.6, 7.8, 9.2, 1.4]\njulia> Pingouin.anderson(x, Normal(1,5))\n(false, 0.04755873570126501)\n\nTest that 2 arrays come from a custom distribution\n\njulia> using Distributions\njulia> x = [[2.3, 5.1, 4.3, 2.6, 7.8], [1, 2.0, 3.5, 5.1]]\njulia> Pingouin.anderson(x, Normal(1,5))\n((true, true), (0.09387808627138938, 0.3413749361130328))\n\n\n\n\n\n","category":"function"},{"location":"distributions/#Pingouin.epsilon-Tuple{Union{DataFrames.DataFrame, Array{var\"#s29\",N} where N where var\"#s29\"<:Number}}","page":"Distributions","title":"Pingouin.epsilon","text":"epsilon(data[, dv, within, subject, correction])\n\nEpsilon adjustement factor for repeated measures.\n\nArguments\n\ndata::Union{Array{<:Number}, DataFrame}: DataFrame containing the repeated measurements. Only long-format dataframe are supported for this function.\ndv::Union{Symbol, String, Nothing}: Name of column containing the dependent variable.\nwithin::Union{Array{String}, Array{Symbol}, Symbol, String, Nothing}: Name of column containing the within factor (only required if data is in long format). If within is a list with two strings, this function computes the epsilon factor for the interaction between the two within-subject factor.\nsubject::Union{Symbol, String, Nothing}: Name of column containing the subject identifier (only required if data is in long format).\ncorrection::String: Specify the epsilon version:\n\"gg\": Greenhouse-Geisser,\n\"hf\": Huynh-Feldt,\n\"lb\": Lower bound.\n\nReturns\n\neps::Float64: Epsilon adjustement factor.\n\nSee Also\n\nsphericity: Mauchly and JNS test for sphericity.\nhomoscedasticity: Test equality of variance.\n\nNotes\n\nThe lower bound epsilon is:\n\nlb = frac1textdof\n\n,\n\nwhere the degrees of freedom textdof is the number of groups k minus 1 for one-way design and (k_1 - 1)(k_2 - 1) for two-way design\n\nThe Greenhouse-Geisser epsilon is given by:\n\nepsilon_GG = frack^2(overlinetextdiag(S) - overlineS)^2(k-1)(sum_i=1^ksum_j=1^ks_ij^2 - 2ksum_j=1^koverlines_i^2 + k^2overlineS^2)\n\nwhere S is the covariance matrix, overlineS the grandmean of S and overlinetextdiag(S) the mean of all the elements on the diagonal of S (i.e. mean of the variances).\n\nThe Huynh-Feldt epsilon is given by:\n\nepsilon_HF = fracn(k-1)epsilon_GG-2(k-1) (n-1-(k-1)epsilon_GG)\n\nwhere n is the number of observations.\n\nMissing values are automatically removed from data (listwise deletion).\n\nExamples\n\nUsing a wide-format dataframe\n\njulia> data = DataFrame(A = [2.2, 3.1, 4.3, 4.1, 7.2],\n                     B = [1.1, 2.5, 4.1, 5.2, 6.4],\n                     C = [8.2, 4.5, 3.4, 6.2, 7.2])\njulia> Pingouin.epsilon(data, correction=\"gg\")\n0.5587754577585018\njulia> Pingouin.epsilon(data, correction=\"hf\")\n0.6223448311539789\njulia> Pingouin.epsilon(data, correction=\"lb\")\n0.5\n\nNow using a long-format dataframe\n\njulia> data = Pingouin.read_dataset(\"rm_anova2\")\njulia> head(data)\n6×4 DataFrame\n│ Row │ Subject │ Time   │ Metric  │ Performance │\n│     │ Int64   │ String │ String  │ Int64       │\n├─────┼─────────┼────────┼─────────┼─────────────┤\n│ 1   │ 1       │ Pre    │ Product │ 13          │\n│ 2   │ 2       │ Pre    │ Product │ 12          │\n│ 3   │ 3       │ Pre    │ Product │ 17          │\n│ 4   │ 4       │ Pre    │ Product │ 12          │\n│ 5   │ 5       │ Pre    │ Product │ 19          │\n│ 6   │ 6       │ Pre    │ Product │ 6           │\n\nLet's first calculate the epsilon of the Time within-subject factor\n\njulia> Pingouin.epsilon(data, dv=\"Performance\", subject=\"Subject\",\n                     within=\"Time\")\n1.0\n\nSince Time has only two levels (Pre and Post), the sphericity assumption is necessarily met, and therefore the epsilon adjustement factor is 1.\n\nThe Metric factor, however, has three levels:\n\njulia> Pingouin.epsilon(data, dv=:Performance, subject=:Subject,\n                     within=[:Metric])\n0.9691029584899762\n\nThe epsilon value is very close to 1, meaning that there is no major violation of sphericity.\n\nNow, let's calculate the epsilon for the interaction between the two repeated measures factor:\n\njulia> Pingouin.epsilon(data, dv=:Performance, subject=:Subject,\n                     within=[:Time, :Metric])\n0.727166420214127\n\n\n\n\n\n","category":"method"},{"location":"distributions/#Pingouin.gzscore-Tuple{Array{var\"#s38\",N} where N where var\"#s38\"<:Number}","page":"Distributions","title":"Pingouin.gzscore","text":"gzscore(x)\n\nGeometric standard (Z) score.\n\nArguments\n\nx::Array{<:Number}: Array of raw values\n\nReturns\n\ngzscore::Array{<:Number}: Array of geometric z-scores (same shape as x)\n\nNotes\n\nGeometric Z-scores are better measures of dispersion than arithmetic z-scores when the sample data come from a log-normally distributed population [1].\n\nGiven the raw scores x, the geometric mean mu_g and the geometric standard deviation sigma_g, the standard score is given by the formula:\n\nz = fraclog(x) - log(mu_g)log(sigma_g)\n\nReferences\n\n[1] https://en.wikipedia.org/wiki/Geometricstandarddeviation\n\nExamples\n\nStandardize a lognormal-distributed vector:\n\njulia> raw = [1,4,5,4,1,2,5,8,6,6,9,8,3]\njulia> z = Pingouin.gzscore(raw)\n13-element Array{Float64,1}:\n -1.8599725059104346\n  0.03137685347921089\n  0.3358161014965816\n  0.03137685347921089\n -1.8599725059104346\n  ⋮\n  0.5845610789821727\n  0.5845610789821727\n  1.1377453044851344\n  0.9770515331740336\n -0.3611136007126501\n\n\n\n\n\n","category":"method"},{"location":"distributions/#Pingouin.homoscedasticity-Tuple{Array{Array{T,1},1} where T<:Number}","page":"Distributions","title":"Pingouin.homoscedasticity","text":"homoscedasticity(data[, dv, group, method, α])\n\nTest equality of variance.\n\nArguments\n\ndata: Iterable. Can be either an Array iterables or a wide- or long-format dataframe.\ndv::Union{Symbol, String, Nothing}: Dependent variable (only when data is a long-format dataframe).\ngroup::Union{Symbol, String, Nothing}: Grouping variable (only when data is a long-format dataframe).\nmethod::String: Statistical test. 'levene' (default) performs the Levene test and 'bartlett' performs the Bartlett test. The former is more robust to departure from normality.\nα::Float64: Significance level.\n\nReturns\n\nstats::DataFrame:\nW/T: Test statistic ('W' for Levene, 'T' for Bartlett), \npval: p-value,\nequal_var: true if data has equal variance.\n\nSee Also\n\nnormality : Univariate normality test.\nsphericity : Mauchly's test for sphericity.\n\nNotes\n\nThe Bartlett T statistic [1] is defined as:\n\nT = frac(N-k) lns^2_p - sum_i=1^k(N_i - 1) lns^2_i1 + (1(3(k-1)))((sum_i=1^k1(N_i - 1)) - 1(N-k))\n\nwhere s_i^2 is the variance of the i^th group, N is the total sample size, N_i is the sample size of the i^th group, k is the number of groups, and s_p^2 is the pooled variance.\n\nThe pooled variance is a weighted average of the group variances and is defined as:\n\ns^2_p = sum_i=1^k(N_i - 1)s^2_i(N-k)\n\nThe p-value is then computed using a chi-square distribution:\n\nT sim chi^2(k-1)\n\nThe Levene W statistic [2] is defined as:\n\nW = frac(N-k) (k-1) fracsum_i=1^kN_i(overlineZ_i-overlineZ)^2  sum_i=1^ksum_j=1^N_i(Z_ij-overlineZ_i)^2 \n\nwhere Z_ij = Y_ij - textmedian(Y_i), overlineZ_i are the group means of Z_ij and overlineZ is the grand mean of Z_ij.\n\nThe p-value is then computed using a F-distribution:\n\nW sim F(k-1 N-k)\n\nWARNING: Missing values are not supported for this function. Make sure to remove them before.\n\nReferences\n\n[1] Bartlett, M. S. (1937). Properties of sufficiency and statistical tests. Proc. R. Soc. Lond. A, 160(901), 268-282.\n\n[2] Brown, M. B., & Forsythe, A. B. (1974). Robust tests for the equality of variances. Journal of the American Statistical Association, 69(346), 364-367.\n\nExamples\n\nLevene test on a wide-format dataframe\n\njulia> data = Pingouin.read_dataset(\"mediation\");\njulia> Pingouin.homoscedasticity(data[[\"X\", \"Y\", \"M\"]])\n1×3 DataFrame\n│ Row │ W       │ pval     │ equal_var │\n│     │ Float64 │ Float64  │ Bool      │\n├─────┼─────────┼──────────┼───────────┤\n│ 1   │ 1.17352 │ 0.310707 │ 1         │\n\nBartlett test using an array of arrays\n\njulia> data = [[4, 8, 9, 20, 14], [5, 8, 15, 45, 12]];\njulia> Pingouin.homoscedasticity(data, method=\"bartlett\", α=.05)\n1×3 DataFrame\n│ Row │ T       │ pval     │ equal_var │\n│     │ Float64 │ Float64  │ Bool      │\n├─────┼─────────┼──────────┼───────────┤\n│ 1   │ 2.87357 │ 0.090045 │ 1         │\n\nLong-format dataframe\n\njulia> data = Pingouin.read_dataset(\"rm_anova2\");\njulia> Pingouin.homoscedasticity(data, \"Performance\", \"Time\")\n1×3 DataFrame\n│ Row │ W       │ pval      │ equal_var │\n│     │ Float64 │ Float64   │ Bool      │\n├─────┼─────────┼───────────┼───────────┤\n│ 1   │ 3.1922  │ 0.0792169 │ 1         │\n\n\n\n\n\n","category":"method"},{"location":"distributions/#Pingouin.normality-Tuple{DataFrames.DataFrame,Union{String, Symbol},Union{String, Symbol}}","page":"Distributions","title":"Pingouin.normality","text":"normality(data[, dv, group, method, α])\n\nUnivariate normality test.\n\nArguments\n\ndata: Iterable. Can be either a single list, 1D array, or a wide- or long-format dataframe.\ndv::Union{Symbol, String, Nothing}: Dependent variable (only when data is a long-format dataframe).\ngroup::Union{Symbol, String, Nothing}: Grouping variable (only when data is a long-format dataframe).\nmethod::String: Normality test. 'shapiro' (default) performs the Shapiro-Wilk test using the AS R94 algorithm. If the kurtosis is higher than 3, it  performs a Shapiro-Francia test for leptokurtic distributions. Supported values: [\"shapiro\", \"jarque_bera\"].\nα::Float64: Significance level.\n\nReturns\n\nstats::DataFrame:\nW: Test statistic,\npval: p-value,\nnormal: true if data is normally distributed.\n\nSee Also\n\nhomoscedasticity: Test equality of variance.\nsphericity: Mauchly's test for sphericity.\n\nNotes\n\nThe Shapiro-Wilk test calculates a :math:W statistic that tests whether a random sample x_1 x_2  x_n comes from a normal distribution.\n\nThe W is normalized (W = (W - μ)  σ)\n\nThe null-hypothesis of this test is that the population is normally distributed. Thus, if the p-value is less than the chosen alpha level (typically set at 0.05), then the null hypothesis is rejected and there is evidence that the data tested are not normally distributed.\n\nThe result of the Shapiro-Wilk test should be interpreted with caution in the case of large sample sizes (>5000). Indeed, quoting from Wikipedia:\n\n\"Like most statistical significance tests, if the sample size is sufficiently large this test may detect even trivial departures from the null hypothesis (i.e., although there may be some statistically significant effect, it may be too small to be of any practical significance); thus, additional investigation of the effect size is typically advisable, e.g., a Q–Q plot in this case.\"\n\nThe Jarque-Bera statistic is to test the null hypothesis that a real-valued vector y is normally distributed. Note that the approximation by the Chi-squared distribution does not work well and the speed of convergence is slow. In small samples, the test tends to be over-sized for nominal levels up to about 3% and under-sized for larger nominal levels (Mantalos, 2010).\n\nNote that missing values are automatically removed (casewise deletion).\n\nReferences\n\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test\n\nfor normality (complete samples). Biometrika, 52(3/4), 591-611.\n\nPanagiotis Mantalos, 2011, \"The three different measures of the sample skewness and\n\nkurtosis and the effects to the Jarque-Bera test for normality\", International Journal of Computational Economics and Econometrics, Vol. 2, No. 1, link.\n\nhttps://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm\nJarque-Bera test on Wikipedia\n\nExamples\n\nShapiro-Wilk test on a 1D array\n\njulia> dataset = Pingouin.read_dataset(\"anova\")\njulia> Pingouin.normality(dataset[\"Pain threshold\"])\n1×3 DataFrame\n│ Row │ W        │ pval     │ normal │\n│     │ Float64  │ Float64  │ Bool   │\n├─────┼──────────┼──────────┼────────┤\n│ 1   │ 0.971204 │ 0.800257 │ 1      │\n\nWide-format dataframe using Jarque-Bera test\n\njulia> dataset = Pingouin.read_dataset(\"mediation\")\njulia> Pingouin.normality(dataset, method=\"jarque_bera\")\n7×4 DataFrame\n│ Row │ dv     │ W        │ pval        │ normal │\n│     │ Symbol │ Float64  │ Float64     │ Bool   │\n├─────┼────────┼──────────┼─────────────┼────────┤\n│ 1   │ X      │ 1.42418  │ 0.490618    │ 1      │\n│ 2   │ M      │ 0.645823 │ 0.724038    │ 1      │\n│ 3   │ Y      │ 0.261805 │ 0.877303    │ 1      │\n│ 4   │ Mbin   │ 16.6735  │ 0.000239553 │ 0      │\n│ 5   │ Ybin   │ 16.6675  │ 0.000240265 │ 0      │\n│ 6   │ W1     │ 5.40923  │ 0.0668961   │ 1      │\n│ 7   │ W2     │ 80.6857  │ 3.01529e-18 │ 0      │\n\nLong-format dataframe\n\njulia> dataset = Pingouin.read_dataset(\"rm_anova2\")\njulia> Pingouin.normality(dataset, :Performance, :Time)\n2×4 DataFrame\n│ Row │ Time   │ W        │ pval      │ normal │\n│     │ String │ Float64  │ Float64   │ Bool   │\n├─────┼────────┼──────────┼───────────┼────────┤\n│ 1   │ Pre    │ 0.967718 │ 0.478771  │ 1      │\n│ 2   │ Post   │ 0.940728 │ 0.0951576 │ 1      │\n\n\n\n\n\n","category":"method"},{"location":"distributions/#Pingouin.sphericity-Tuple{DataFrames.DataFrame}","page":"Distributions","title":"Pingouin.sphericity","text":"sphericity(data[, dv, within, subject, method, α])\n\nMauchly and JNS test for sphericity.\n\nArguments\n\ndata::DataFrame: DataFrame containing the repeated measurements. Only long-format dataframe are supported for this function.\ndv::Union{Nothing, String, Symbol}: Name of column containing the dependent variable.\nwithin::Union{Array{String}, Array{Symbol}, Nothing, String, Symbol}: Name of column containing the within factor. If within is a list with two strings, this function computes the epsilon factor for the interaction between the two within-subject factor.\nsubject::Union{Nothing, String, Symbol}: Name of column containing the subject identifier (only required if data is in long format).\nmethod::String: Method to compute sphericity:\n\"jns\": John, Nagao and Sugiura test.\n\"mauchly\": Mauchly test (default).\nα::Float64: Significance level\n\nReturns\n\nspher::Bool: True if data have the sphericity property.\nW::Float64: Test statistic.\nchi2::Float64: Chi-square statistic.\ndof::Int64:  Degrees of freedom.\npval::Flot64: P-value.\n\nThrowing\n\nDomainError When testing for an interaction, if both within-subject factors have more than 2 levels (not yet supported in Pingouin).\n\nSee Also\n\nepsilon: Epsilon adjustement factor for repeated measures.\nhomoscedasticity: Test equality of variance.\nnormality: Univariate normality test.\n\nNotes\n\nThe Mauchly W statistic [1] is defined by:\n\nW = fracprod lambda_j(frac1k-1 sum lambda_j)^k-1\n\nwhere lambda_j are the eigenvalues of the population covariance matrix (= double-centered sample covariance matrix) and k is the number of conditions.\n\nFrom then, the W statistic is transformed into a chi-square score using the number of observations per condition n\n\nf = frac2(k-1)^2+k+16(k-1)(n-1)\n\nchi_w^2 = (f-1)(n-1) textlog(W)\n\nThe p-value is then approximated using a chi-square distribution:\n\nchi_w^2 sim chi^2(frack(k-1)2-1)\n\nThe JNS V statistic ([2], [3], [4]) is defined by:\n\nV = frac(sum_j^k-1 lambda_j)^2sum_j^k-1 lambda_j^2\n\nchi_v^2 = fracn2  (k-1)^2 (V - frac1k-1)\n\nand the p-value approximated using a chi-square distribution\n\nchi_v^2 sim chi^2(frack(k-1)2-1)\n\nMissing values are automatically removed from data (listwise deletion).\n\nReferences\n\n[1] Mauchly, J. W. (1940). Significance test for sphericity of a normal n-variate distribution. The Annals of Mathematical Statistics, 11(2), 204-209.\n\n[2] Nagao, H. (1973). On some test criteria for covariance matrix. The Annals of Statistics, 700-709.\n\n[3] Sugiura, N. (1972). Locally best invariant test for sphericity and the limiting distributions. The Annals of Mathematical Statistics, 1312-1316.\n\n[4] John, S. (1972). The distribution of a statistic used for testing sphericity of normal distributions. Biometrika, 59(1), 169-173.\n\nSee also http://www.real-statistics.com/anova-repeated-measures/sphericity/\n\nExamples\n\nMauchly test for sphericity using a wide-format dataframe\n\njulia> data = DataFrame(A = [2.2, 3.1, 4.3, 4.1, 7.2],\n                     B = [1.1, 2.5, 4.1, 5.2, 6.4],\n                     C = [8.2, 4.5, 3.4, 6.2, 7.2])\njulia> Pingouin.sphericity(data)\n│ Row │ spher │ W        │ chi2    │ dof     │ pval      │\n│     │ Bool  │ Float64  │ Float64 │ Float64 │ Float64   │\n├─────┼───────┼──────────┼─────────┼─────────┼───────────┤\n│ 1   │ 1     │ 0.210372 │ 4.67663 │ 2.0     │ 0.0964902 │\n\nJohn, Nagao and Sugiura (JNS) test\n\njulia> Pingouin.sphericity(data, method=\"jns\")[:pval]  # P-value only\n0.045604240307520305\n\nNow using a long-format dataframe\n\njulia> data = Pingouin.read_dataset(\"rm_anova2\")\njulia> head(data)\n6x4 DataFrame\n│ Row │ Subject │ Time   │ Metric  │ Performance │\n│     │ Int64   │ String │ String  │ Int64       │\n├─────┼─────────┼────────┼─────────┼─────────────┤\n│ 1   │ 1       │ Pre    │ Product │ 13          │\n│ 2   │ 2       │ Pre    │ Product │ 12          │\n│ 3   │ 3       │ Pre    │ Product │ 17          │\n│ 4   │ 4       │ Pre    │ Product │ 12          │\n│ 5   │ 5       │ Pre    │ Product │ 19          │\n│ 6   │ 6       │ Pre    │ Product │ 6           │\n\nLet's first test sphericity for the Time within-subject factor\n\njulia> Pingouin.sphericity(data, dv=:Performance, subject=:Subject,\n                        within=:Time)\n│ Row │ spher │ W       │ chi2    │ dof   │ pval    │\n│     │ Bool  │ Float64 │ Float64 │ Int64 │ Float64 │\n├─────┼───────┼─────────┼─────────┼───────┼─────────┤\n│ 1   │ 1     │ NaN     │ NaN     │ 1     │ 1.0     │\n\nSince Time has only two levels (Pre and Post), the sphericity assumption is necessarily met.\n\nThe Metric factor, however, has three levels:\n\njulia> Pingouin.sphericity(data, dv=\"Performance\", subject=\"Subject\",\n                        within=[\"Metric\"])[:pval]\n0.8784417991645139\n\nThe p-value value is very large, and the test therefore indicates that there is no violation of sphericity.\n\nNow, let's calculate the epsilon for the interaction between the two repeated measures factor. The current implementation in Pingouin only works if at least one of the two within-subject factors has no more than two levels.\n\njulia> Pingouin.sphericity(data, dv=\"Performance\",\n                        subject=\"Subject\",\n                        within=[\"Time\", \"Metric\"])\n│ Row │ spher │ W        │ chi2    │ dof     │ pval     │\n│     │ Bool  │ Float64  │ Float64 │ Float64 │ Float64  │\n├─────┼───────┼──────────┼─────────┼─────────┼──────────┤\n│ 1   │ 1     │ 0.624799 │ 3.7626  │ 2.0     │ 0.152392 │\n\nHere again, there is no violation of sphericity acccording to Mauchly's test.\n\n\n\n\n\n","category":"method"},{"location":"effsize/#Effect-sizes","page":"Effect Sizes","title":"Effect sizes","text":"","category":"section"},{"location":"effsize/","page":"Effect Sizes","title":"Effect Sizes","text":"A bunch of functions to compte effect sizes, and convert them.","category":"page"},{"location":"effsize/","page":"Effect Sizes","title":"Effect Sizes","text":"Modules = [Pingouin]\nPages   = [\"effsize.jl\"]","category":"page"},{"location":"effsize/#Pingouin.compute_bootci-Tuple{Array{var\"#s12\",N} where N where var\"#s12\"<:Number}","page":"Effect Sizes","title":"Pingouin.compute_bootci","text":"compute_bootci(x[, y, func, method, paired, confidence, n_boot, decimals, seed, return_dist])\n\nBootstrapped confidence intervals of univariate and bivariate functions.\n\nArguments\n\nx::Array{<:Number}: First sample. Required for both bivariate and univariate functions.\ny::Array{<:Number}: Second sample. Required only for bivariate functions.\nfunc::Union{Function, String}: Function to compute the bootstrapped statistic. Accepted string values are:\n'pearson': Pearson correlation (bivariate, requires x and y)\n'spearman': Spearman correlation (bivariate)\n'cohen': Cohen d effect size (bivariate)\n'hedges': Hedges g effect size (bivariate)\n'mean': Mean (univariate, requires only x)\n'std': Standard deviation (univariate)\n'var': Variance (univariate)\nmethod::String: Method to compute the confidence intervals:\n'norm': Normal approximation with bootstrapped bias and standard error\n'per': Basic percentile method\n'cper': Bias corrected percentile method (default)\npaired::Bool: Indicates whether x and y are paired or not. Only useful when computing bivariate Cohen d or Hedges g bootstrapped confidence intervals.\nconfidence::Float64: Confidence level (0.95 = 95%)\nn_boot::Int64: Number of bootstrap iterations. The higher, the better, the slower.\ndecimals::Int64: Number of rounded decimals.\nseed::Int64: Random seed for generating bootstrap samples.\nreturn_dist::Bool: If True, return the confidence intervals and the bootstrapped distribution  (e.g. for plotting purposes).\n\nReturns\n\nci::Array{<:Number}: Desired converted effect size\n\nNotes\n\nResults have been tested against the bootci Matlab function.\n\nReferences\n\nDiCiccio, T. J., & Efron, B. (1996). Bootstrap confidence intervals.\n\nStatistical science, 189-212.\n\nDavison, A. C., & Hinkley, D. V. (1997). Bootstrap methods and their\n\napplication (Vol. 1). Cambridge university press.\n\nExamples\n\nBootstrapped 95% confidence interval of a Pearson correlation\n\njulia> x = [3, 4, 6, 7, 5, 6, 7, 3, 5, 4, 2]\njulia> y = [4, 6, 6, 7, 6, 5, 5, 2, 3, 4, 1]\njulia> stat = cor(x, y)\n0.7468280049029223\njulia> ci = Pingouin.compute_bootci(x=x, y=y, func=\"pearson\", seed=42)\n2-element Array{Float64,1}:\n 0.22\n 0.93\n\nBootstrapped 95% confidence interval of a Cohen d\n\njulia> stat = Pingouin.compute_effsize(x, y, eftype=\"cohen\")\n0.1537753990658328\njulia> ci = Pingouin.compute_bootci(x, y=y, func=\"cohen\", seed=42, decimals=3)\n2-element Array{Float64,1}:\n -0.329\n  0.589\n\nBootstrapped confidence interval of a standard deviation (univariate)\n\njulia> stat = std(x)\n1.6787441193290351\njulia> ci = Pingouin.compute_bootci(x, func=\"std\", seed=123)\n2-element Array{Float64,1}:\n 1.25\n 2.2\n\nBootstrapped confidence interval using a custom univariate function\n\njulia> skewness(x), Pingouin.compute_bootci(x, func=skewness, n_boot=10000, seed=123)\n(-0.08244607271328411, [-1.01, 0.77])\n\nBootstrapped confidence interval using a custom bivariate function\n\njulia> stat = sum(exp.(x) ./ exp.(y))\n26.80405184881793\njulia> ci = Pingouin.compute_bootci(x, y=y, func=f(x, y) = sum(exp.(x) ./ exp.(y)), n_boot=10000, seed=123)\njulia> print(stat, ci)\n2-element Array{Float64,1}:\n 12.76\n 45.52\n\nGet the bootstrapped distribution around a Pearson correlation\n\njulia> ci, bstat = Pingouin.compute_bootci(x, y=y, return_dist=true)\n([0.27, 0.92], [0.6661370089058535, ...])\n\n\n\n\n\n","category":"method"},{"location":"effsize/#Pingouin.compute_bootci-Tuple{Array{var\"#s38\",N} where N where var\"#s38\"<:Number}","page":"Effect Sizes","title":"Pingouin.compute_bootci","text":"compute_bootci(x[, y, func, method, paired, confidence, n_boot, decimals, seed, return_dist])\n\nBootstrapped confidence intervals of univariate and bivariate functions.\n\nArguments\n\nx::Array{<:Number}: First sample. Required for both bivariate and univariate functions.\ny::Array{<:Number}: Second sample. Required only for bivariate functions.\nfunc::Union{Function, String}: Function to compute the bootstrapped statistic. Accepted string values are:\n'pearson': Pearson correlation (bivariate, requires x and y)\n'spearman': Spearman correlation (bivariate)\n'cohen': Cohen d effect size (bivariate)\n'hedges': Hedges g effect size (bivariate)\n'mean': Mean (univariate, requires only x)\n'std': Standard deviation (univariate)\n'var': Variance (univariate)\nmethod::String: Method to compute the confidence intervals:\n'norm': Normal approximation with bootstrapped bias and standard error\n'per': Basic percentile method\n'cper': Bias corrected percentile method (default)\npaired::Bool: Indicates whether x and y are paired or not. Only useful when computing bivariate Cohen d or Hedges g bootstrapped confidence intervals.\nconfidence::Float64: Confidence level (0.95 = 95%)\nn_boot::Int64: Number of bootstrap iterations. The higher, the better, the slower.\ndecimals::Int64: Number of rounded decimals.\nseed::Int64: Random seed for generating bootstrap samples.\nreturn_dist::Bool: If True, return the confidence intervals and the bootstrapped distribution  (e.g. for plotting purposes).\n\nReturns\n\nci::Array{<:Number}: Desired converted effect size\n\nNotes\n\nResults have been tested against the bootci Matlab function.\n\nReferences\n\nDiCiccio, T. J., & Efron, B. (1996). Bootstrap confidence intervals.\n\nStatistical science, 189-212.\n\nDavison, A. C., & Hinkley, D. V. (1997). Bootstrap methods and their\n\napplication (Vol. 1). Cambridge university press.\n\nExamples\n\nBootstrapped 95% confidence interval of a Pearson correlation\n\njulia> x = [3, 4, 6, 7, 5, 6, 7, 3, 5, 4, 2]\njulia> y = [4, 6, 6, 7, 6, 5, 5, 2, 3, 4, 1]\njulia> stat = cor(x, y)\n0.7468280049029223\njulia> ci = Pingouin.compute_bootci(x=x, y=y, func=\"pearson\", seed=42)\n2-element Array{Float64,1}:\n 0.22\n 0.93\n\nBootstrapped 95% confidence interval of a Cohen d\n\njulia> stat = Pingouin.compute_effsize(x, y, eftype=\"cohen\")\n0.1537753990658328\njulia> ci = Pingouin.compute_bootci(x, y=y, func=\"cohen\", seed=42, decimals=3)\n2-element Array{Float64,1}:\n -0.329\n  0.589\n\nBootstrapped confidence interval of a standard deviation (univariate)\n\njulia> stat = std(x)\n1.6787441193290351\njulia> ci = Pingouin.compute_bootci(x, func=\"std\", seed=123)\n2-element Array{Float64,1}:\n 1.25\n 2.2\n\nBootstrapped confidence interval using a custom univariate function\n\njulia> skewness(x), Pingouin.compute_bootci(x, func=skewness, n_boot=10000, seed=123)\n(-0.08244607271328411, [-1.01, 0.77])\n\nBootstrapped confidence interval using a custom bivariate function\n\njulia> stat = sum(exp.(x) ./ exp.(y))\n26.80405184881793\njulia> ci = Pingouin.compute_bootci(x, y=y, func=f(x, y) = sum(exp.(x) ./ exp.(y)), n_boot=10000, seed=123)\njulia> print(stat, ci)\n2-element Array{Float64,1}:\n 12.76\n 45.52\n\nGet the bootstrapped distribution around a Pearson correlation\n\njulia> ci, bstat = Pingouin.compute_bootci(x, y=y, return_dist=true)\n([0.27, 0.92], [0.6661370089058535, ...])\n\n\n\n\n\n","category":"method"},{"location":"effsize/#Pingouin.compute_effsize-Tuple{Array{var\"#s170\",N} where N where var\"#s170\"<:Number,Array{var\"#s169\",N} where N where var\"#s169\"<:Number}","page":"Effect Sizes","title":"Pingouin.compute_effsize","text":"compute_effsize(x, y[, paired, eftype])\n\nCalculate effect size between two set of observations.\n\nArguments\n\nx::Array{<:Number}: First set of observations.\ny::Array{<:Number}: Second set of observations.\npaired::Bool: If True, uses Cohen d-avg formula to correct for repeated measurements (see Notes).\neftype::String: Desired output effect size. Available methods are:\n\"none\": no effect size\n\"cohen\": Unbiased Cohen d\n\"hedges\": Hedges g\n\"glass\": Glass delta\n\"r\": correlation coefficient\n\"eta-square\": Eta-square\n\"odds-ratio\": Odds ratio\n\"auc\": Area Under the Curve\n\"cles\": Common Language Effect Size\n\nReturns\n\nef::Float64: Effect size\n\nSee Also\n\nconvert_effsize: Conversion between effect sizes.\ncompute_effsize_from_t: Convert a T-statistic to an effect size.\n\nNotes\n\nMissing values are automatically removed from the data. If x and y are paired, the entire row is removed.\n\nIf x and y are independent, the Cohen d is:\n\nd = fracoverlineX - overlineY sqrtfrac(n_1 - 1)sigma_1^2 + (n_2 - 1) sigma_2^2n1 + n2 - 2\n\nIf x and y are paired, the Cohen d_avg is computed:\n\nd_avg = fracoverlineX - overlineY sqrtfrac(sigma_1^2 + sigma_2^2)2\n\nThe Cohen’s d is a biased estimate of the population effect size, especially for small samples (n < 20). It is often preferable to use the corrected Hedges g instead:\n\ng = d times (1 - frac34(n_1 + n_2) - 9)\n\nThe Glass delta is calculated using the group with the lowest variance as the control group:\n\ndelta = fracoverlineX - overlineYsigma^2_textcontrol\n\nThe common language effect size is the proportion of pairs where x is higher than y (calculated with a brute-force approach where each observation of x is paired to each observation of y, see wilcoxon for more details):\n\ntextCL = P(X  Y) + 5 times P(X = Y)\n\nFor other effect sizes, Pingouin will first calculate a Cohen d and then use the convert_effsize to convert to the desired effect size.\n\nReferences\n\nLakens, D., 2013. Calculating and reporting effect sizes to\n\nfacilitate cumulative science: a practical primer for t-tests and ANOVAs. Front. Psychol. 4, 863. https://doi.org/10.3389/fpsyg.2013.00863\n\nCumming, Geoff. Understanding the new statistics: Effect sizes,\n\nconfidence intervals, and meta-analysis. Routledge, 2013.\n\nhttps://osf.io/vbdah/\n\nExamples\n\nCohen d from two independent samples.\n\njulia> x = [1, 2, 3, 4]\njulia> y = [3, 4, 5, 6, 7]\njulia> Pingouin.compute_effsize(x, y, paired=false, eftype=\"cohen\")\n-1.707825127659933\n\nThe sign of the Cohen d will be opposite if we reverse the order of x and y:\n\njulia> Pingouin.compute_effsize(y, x, paired=false, eftype=\"cohen\")\n1.707825127659933\n\nHedges g from two paired samples.\n\njulia> x = [1, 2, 3, 4, 5, 6, 7]\njulia> y = [1, 3, 5, 7, 9, 11, 13]\njulia> Pingouin.compute_effsize(x, y, paired=true, eftype=\"hedges\")\n-0.8222477210374874\n\nGlass delta from two independent samples. The group with the lowest\n\nvariance will automatically be selected as the control.\n\njulia> Pingouin.compute_effsize(x, y, paired=false, eftype=\"glass\")\n-1.3887301496588271\n\nCommon Language Effect Size.\n\njulia> Pingouin.compute_effsize(x, y, eftype=\"cles\")\n0.2857142857142857\n\nIn other words, there are ~29% of pairs where x is higher than y, which means that there are ~71% of pairs where x is lower than y. This can be easily verified by changing the order of x and y:\n\njulia> Pingouin.compute_effsize(y, x, eftype=\"cles\")\n0.7142857142857143\n\n\n\n\n\n","category":"method"},{"location":"effsize/#Pingouin.compute_effsize-Tuple{Array{var\"#s29\",N} where N where var\"#s29\"<:Number,Array{var\"#s28\",N} where N where var\"#s28\"<:Number}","page":"Effect Sizes","title":"Pingouin.compute_effsize","text":"compute_effsize(x, y[, paired, eftype])\n\nCalculate effect size between two set of observations.\n\nArguments\n\nx::Array{<:Number}: First set of observations.\ny::Array{<:Number}: Second set of observations.\npaired::Bool: If True, uses Cohen d-avg formula to correct for repeated measurements (see Notes).\neftype::String: Desired output effect size. Available methods are:\n\"none\": no effect size\n\"cohen\": Unbiased Cohen d\n\"hedges\": Hedges g\n\"glass\": Glass delta\n\"r\": correlation coefficient\n\"eta-square\": Eta-square\n\"odds-ratio\": Odds ratio\n\"auc\": Area Under the Curve\n\"cles\": Common Language Effect Size\n\nReturns\n\nef::Float64: Effect size\n\nSee Also\n\nconvert_effsize: Conversion between effect sizes.\ncompute_effsize_from_t: Convert a T-statistic to an effect size.\n\nNotes\n\nMissing values are automatically removed from the data. If x and y are paired, the entire row is removed.\n\nIf x and y are independent, the Cohen d is:\n\nd = fracoverlineX - overlineY sqrtfrac(n_1 - 1)sigma_1^2 + (n_2 - 1) sigma_2^2n1 + n2 - 2\n\nIf x and y are paired, the Cohen d_avg is computed:\n\nd_avg = fracoverlineX - overlineY sqrtfrac(sigma_1^2 + sigma_2^2)2\n\nThe Cohen’s d is a biased estimate of the population effect size, especially for small samples (n < 20). It is often preferable to use the corrected Hedges g instead:\n\ng = d times (1 - frac34(n_1 + n_2) - 9)\n\nThe Glass delta is calculated using the group with the lowest variance as the control group:\n\ndelta = fracoverlineX - overlineYsigma^2_textcontrol\n\nThe common language effect size is the proportion of pairs where x is higher than y (calculated with a brute-force approach where each observation of x is paired to each observation of y, see wilcoxon for more details):\n\ntextCL = P(X  Y) + 5 times P(X = Y)\n\nFor other effect sizes, Pingouin will first calculate a Cohen d and then use the convert_effsize to convert to the desired effect size.\n\nReferences\n\nLakens, D., 2013. Calculating and reporting effect sizes to\n\nfacilitate cumulative science: a practical primer for t-tests and ANOVAs. Front. Psychol. 4, 863. https://doi.org/10.3389/fpsyg.2013.00863\n\nCumming, Geoff. Understanding the new statistics: Effect sizes,\n\nconfidence intervals, and meta-analysis. Routledge, 2013.\n\nhttps://osf.io/vbdah/\n\nExamples\n\nCohen d from two independent samples.\n\njulia> x = [1, 2, 3, 4]\njulia> y = [3, 4, 5, 6, 7]\njulia> Pingouin.compute_effsize(x, y, paired=false, eftype=\"cohen\")\n-1.707825127659933\n\nThe sign of the Cohen d will be opposite if we reverse the order of x and y:\n\njulia> Pingouin.compute_effsize(y, x, paired=false, eftype=\"cohen\")\n1.707825127659933\n\nHedges g from two paired samples.\n\njulia> x = [1, 2, 3, 4, 5, 6, 7]\njulia> y = [1, 3, 5, 7, 9, 11, 13]\njulia> Pingouin.compute_effsize(x, y, paired=true, eftype=\"hedges\")\n-0.8222477210374874\n\nGlass delta from two independent samples. The group with the lowest\n\nvariance will automatically be selected as the control.\n\njulia> Pingouin.compute_effsize(x, y, paired=false, eftype=\"glass\")\n-1.3887301496588271\n\nCommon Language Effect Size.\n\njulia> Pingouin.compute_effsize(x, y, eftype=\"cles\")\n0.2857142857142857\n\nIn other words, there are ~29% of pairs where x is higher than y, which means that there are ~71% of pairs where x is lower than y. This can be easily verified by changing the order of x and y:\n\njulia> Pingouin.compute_effsize(y, x, eftype=\"cles\")\n0.7142857142857143\n\n\n\n\n\n","category":"method"},{"location":"effsize/#Pingouin.compute_effsize_from_t-Tuple{Float64}","page":"Effect Sizes","title":"Pingouin.compute_effsize_from_t","text":"compute_effsize_from_t(tval[, nx, ny, N, eftype])\n\nCompute effect size from a T-value.\n\nParameters\n\ntval::Float64: T-value.\nnx, ny::Int64: Optional. Group sample sizes.\nN::Int64: Optional. Total sample size (will not be used if nx and ny are specified).\neftype::String: Optional. Desired output effect size.\n\nReturns\n\nef::Float64: Effect size\n\nSee Also\n\ncompute_effsize: Calculate effect size between two set of observations.\nconvert_effsize: Conversion between effect sizes.\n\nNotes\n\nIf both nx and ny are specified, the formula to convert from t to d is:\n\nd = t * sqrtfrac1n_x + frac1n_y\n\nIf only N (total sample size) is specified, the formula is:\n\nd = frac2tsqrtN\n\nExamples\n\nCompute effect size from a T-value when both sample sizes are known.\n\njulia> tval, nx, ny = 2.90, 35, 25\njulia> d = Pingouin.compute_effsize_from_t(tval, nx=nx, ny=ny, eftype=\"cohen\")\n0.7593982580212534\n\nCompute effect size when only total sample size is known (nx+ny)\n\njulia> tval, N = 2.90, 60\njulia> d = Pingouin.compute_effsize_from_t(tval, N=N, eftype=\"cohen\")\n0.7487767802667672\n\n\n\n\n\n","category":"method"},{"location":"effsize/#Pingouin.compute_esci-Tuple{}","page":"Effect Sizes","title":"Pingouin.compute_esci","text":"compute_esci(stat, nx, ny[, paired, eftype, confidence, decimals])\n\nParametric confidence intervals around a Cohen d or a correlation coefficient.\n\nArguments\n\nstat::Float64: Original effect size. Must be either a correlation coefficient or a Cohen-type effect size (Cohen d or Hedges g).\nnx, ny::Int64: Length of vector x and y.\npaired::Bool: Indicates if the effect size was estimated from a paired sample. This is only relevant for cohen or hedges effect size.\neftype::String: Effect size type. Must be \"r\" (correlation) or \"cohen\" (Cohen d or Hedges g).\nconfidence::Float64: Confidence level (0.95 = 95%)\ndecimals::Int64: Number of rounded decimals.\n\nReturns\n\nci::Array Desired converted effect size\n\nNotes\n\nTo compute the parametric confidence interval around a Pearson r correlation coefficient, one must first apply a Fisher's r-to-z transformation:\n\nz = 05 cdot ln frac1 + r1 - r = textarctanh(r)\n\nand compute the standard deviation:\n\nsigma = frac1sqrtn - 3\n\nwhere n is the sample size.\n\nThe lower and upper confidence intervals - in z-space - are then given by:\n\ntextci_z = z pm textcrit cdot sigma\n\nwhere textcrit is the critical value of the normal distribution corresponding to the desired confidence level (e.g. 1.96 in case of a 95% confidence interval).\n\nThese confidence intervals can then be easily converted back to r-space:\n\ntextci_r = fracexp(2 cdot textci_z) - 1 exp(2 cdot textci_z) + 1 = texttanh(textci_z)\n\nA formula for calculating the confidence interval for a Cohen d effect size is given by Hedges and Olkin (1985, p86). If the effect size estimate from the sample is d, then it follows a T distribution with standard deviation:\n\nsigma = sqrtfracn_x + n_yn_x cdot n_y + fracd^22 (n_x + n_y)\n\nwhere n_x and n_y are the sample sizes of the two groups.\n\nIn one-sample test or paired test, this becomes:\n\nsigma = sqrtfrac1n_x + fracd^22 n_x\n\nThe lower and upper confidence intervals are then given by:\n\ntextci_d = d pm textcrit cdot sigma\n\nwhere textcrit is the critical value of the T distribution corresponding to the desired confidence level.\n\nReferences\n\nhttps://en.wikipedia.org/wiki/Fisher_transformation\nHedges, L., and Ingram Olkin. \"Statistical models for meta-analysis.\"   (1985).\nhttp://www.leeds.ac.uk/educol/documents/00002182.htm\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5133225/\n\nExamples\n\nConfidence interval of a Pearson correlation coefficient\n\njulia> x = [3, 4, 6, 7, 5, 6, 7, 3, 5, 4, 2]\njulia> y = [4, 6, 6, 7, 6, 5, 5, 2, 3, 4, 1]\njulia> nx, ny = length(x), length(y)\njulia> stat = Pingouin.compute_effsize(x, y, eftype=\"r\")\n0.7468280049029223\njulia> ci = Pingouin.compute_esci(stat=stat, nx=nx, ny=ny, eftype=\"r\")\n2-element Array{Float64,1}:\n 0.27\n 0.93\n\nConfidence interval of a Cohen d\n\njulia> stat = Pingouin.compute_effsize(x, y, eftype=\"cohen\")\n0.1537753990658328\njulia> ci = Pingouin.compute_esci(stat=stat, nx=nx, ny=ny, eftype=\"cohen\", decimals=3)\n2-element Array{Float64,1}:\n -0.737\n  1.045\n\n\n\n\n\n","category":"method"},{"location":"effsize/#Pingouin.convert_effsize-Tuple{Float64,String,String}","page":"Effect Sizes","title":"Pingouin.convert_effsize","text":"convert_effsize(ef, input_type, output_type[, nx, ny])\n\nConversion between effect sizes.\n\nParameters\n\nef::Float64: Original effect size.\ninput_type::String: Effect size type of ef. Must be \"r\" or \"d\".\noutput_type::String: Desired effect size type.\nnx, ny::Int64: Optional. Length of vector x and y. Required to convert to Hedges g. Available methods are:\n\"cohen\": Unbiased Cohen d\n\"hedges\": Hedges g\n\"eta-square\": Eta-square\n\"odds-ratio\": Odds ratio\n\"AUC\": Area Under the Curve\n\"none\": pass-through (return ef)\n\nReturns\n\nef::Float64: Desired converted effect size\n\nSee Also\n\ncompute_effsize: Calculate effect size between two set of observations.\ncompute_effsize_from_t: Convert a T-statistic to an effect size.\n\nNotes\n\nThe formula to convert r to d is given in [1]:\n\nd = frac2rsqrt1 - r^2\n\nThe formula to convert d to r is given in [2]:\n\nr = fracdsqrtd^2 + frac(n_x + n_y)^2 - 2(n_x + n_y) n_xn_y\n\nThe formula to convert d to eta^2 is given in [3]:\n\neta^2 = frac(05 d)^21 + (05 d)^2\n\nThe formula to convert d to an odds-ratio is given in [4]:\n\ntextOR = exp (fracd pisqrt3)\n\nThe formula to convert d to area under the curve is given in [5]:\n\ntextAUC = mathcalN_cdf(fracdsqrt2)\n\nReferences\n\n[1] Rosenthal, Robert. \"Parametric measures of effect size.\" The handbook of research synthesis 621 (1994): 231-244.\n\n[2] McGrath, Robert E., and Gregory J. Meyer. \"When effect sizes disagree: the case of r and d.\" Psychological methods 11.4 (2006): 386.\n\n[3] Cohen, Jacob. \"Statistical power analysis for the behavioral sciences. 2nd.\" (1988).\n\n[4] Borenstein, Michael, et al. \"Effect sizes for continuous data.\" The handbook of research synthesis and meta-analysis 2 (2009): 221-235.\n\n[5] Ruscio, John. \"A probability-based measure of effect size: Robustness to base rates and other factors.\" Psychological methods 1 3.1 (2008): 19.\n\nExamples\n\nConvert from Cohen d to eta-square\n\njulia> d = .45\njulia> eta = Pingouin.convert_effsize(d, \"cohen\", \"eta-square\")\n0.048185603807257595\n\nConvert from Cohen d to Hegdes g (requires the sample sizes of each\n\ngroup)\n\njulia> Pingouin.convert_effsize(.45, \"cohen\", \"hedges\", nx=10, ny=10)\n0.4309859154929578\n\nConvert Pearson r to Cohen d\n\njulia> r = 0.40\njulia> d = Pingouin.convert_effsize(r, \"r\", \"cohen\")\n0.8728715609439696\n\nReverse operation: convert Cohen d to Pearson r\n\njulia> Pingouin.convert_effsize(d, \"cohen\", \"r\")\n0.4000000000000001\n\n\n\n\n\n","category":"method"},{"location":"#Pingouin.jl-Documentation","page":"Pingouin.jl Documentation","title":"Pingouin.jl Documentation","text":"","category":"section"},{"location":"","page":"Pingouin.jl Documentation","title":"Pingouin.jl Documentation","text":"Documentation for Pingouin.jl. Pingouin is a simple yet exhaustive statistical library, ported from Raphael Vallat's Pingouin.","category":"page"},{"location":"#Index","page":"Pingouin.jl Documentation","title":"Index","text":"","category":"section"},{"location":"","page":"Pingouin.jl Documentation","title":"Pingouin.jl Documentation","text":"","category":"page"},{"location":"datasets/#Datasets","page":"Datasets","title":"Datasets","text":"","category":"section"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"Loading and using a set of pre-included datasets.","category":"page"},{"location":"datasets/","page":"Datasets","title":"Datasets","text":"Modules = [Pingouin]\nPages   = [\"datasets.jl\"]","category":"page"},{"location":"datasets/#Pingouin.list_dataset-Tuple{}","page":"Datasets","title":"Pingouin.list_dataset","text":"list_dataset()\n\nList available example datasets.\n\nReturns\n\ndatasets : DataFrame: A dataframe with the name, description and reference of all the datasets included in Pingouin.\n\nExamples\n\njulia> all_datasets = Pingouin.list_dataset()\n28×4 DataFrame. Omitted printing of 1 columns\n│ Row │ dataset    │ description                                                                                                        │ useful                 │\n│     │ String     │ String                                                                                                             │ String                 │\n├─────┼────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼────────────────────────┤\n│ 1   │ ancova     │ Teaching method with family income as covariate                                                                    │ ANCOVA                 │\n│ 2   │ anova      │ Pain threshold per hair color                                                                                      │ anova - pairwise_tukey │\n│ 3   │ anova2     │ Fertilizer impact on the yield of crops                                                                            │ anova                  │\n⋮\n│ 25  │ rm_anova2  │ Performance of employees at two time points and three areas                                                        │ rm_anova2              │\n│ 26  │ rm_corr    │ Repeated measurements of pH and PaCO2                                                                              │ rm_corr                │\n│ 27  │ rm_missing │ Missing values in long-format repeated measures dataframe                                                          │ rm_anova - rm_anova2   │\n│ 28  │ tips       │ One waiter recorded information about each tip he received over a period of a few months working in one restaurant │ regression             │\n\n\n\n\n\n","category":"method"},{"location":"datasets/#Pingouin.read_dataset-Tuple{String}","page":"Datasets","title":"Pingouin.read_dataset","text":"read_dataset(dname)\n\nRead example datasets.\n\nArguments\n\ndname::String: Name of dataset to read (without extension). Must be a valid dataset present in Pingouin.datasets\n\nReturns\n\ndata : DataFrame: Requested dataset.\n\nExamples\n\nLoad the Penguin <https://github.com/allisonhorst/palmerpenguins> dataset:\n\njulia> data = Pingouin.read_dataset(\"penguins\")\n344×7 DataFrame\n│ Row │ species │ island │ bill_length_mm │ bill_depth_mm │ flipper_length_mm │ body_mass_g │ sex    │\n│     │ String  │ String │ String         │ String        │ String            │ String      │ String │\n├─────┼─────────┼────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┤\n│ 1   │ Adelie  │ Biscoe │ 37.8           │ 18.3          │ 174               │ 3400        │ female │\n│ 2   │ Adelie  │ Biscoe │ 37.7           │ 18.7          │ 180               │ 3600        │ male   │\n│ 3   │ Adelie  │ Biscoe │ 35.9           │ 19.2          │ 189               │ 3800        │ female │\n⋮\n│ 341 │ Gentoo  │ Biscoe │ 46.8           │ 14.3          │ 215               │ 4850        │ female │\n│ 342 │ Gentoo  │ Biscoe │ 50.4           │ 15.7          │ 222               │ 5750        │ male   │\n│ 343 │ Gentoo  │ Biscoe │ 45.2           │ 14.8          │ 212               │ 5200        │ female │\n│ 344 │ Gentoo  │ Biscoe │ 49.9           │ 16.1          │ 213               │ 5400        │ male   │\n\n\n\n\n\n","category":"method"}]
}
